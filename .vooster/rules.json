{
  "rules": [
    {
      "type": "prd",
      "content": "# 📘 제품 요구사항 문서 (PRD)\n\n## 1. 개요\n\n- **제품 이름:** ReBridge\n    \n- **작성 일자:** 2025-07-12 (rev. MVP-02)\n    \n- **문서 목적:** 정신장애인 맞춤 구직 정보 통합 플랫폼의 **MVP** 기능‧기술 요구사항을 정의한다. MVP는 “구인 정보 수집·제공”에 집중해 **4주 내 실사용** 가능한 최소 제품을 목표로 한다.\n    \n\n---\n\n## 2. 제품 목표 (MVP 관점)\n\n1. **채용 정보 접근성 극대화** – 주요 4개 채널의 공고를 매일 자동 수집‧정규화‧검색 제공.\n    \n2. **개인화 기반 저장·알림** – 사용자는 관심 공고를 저장하고, 새·마감 공고 알림을 수신한다.\n    \n3. **단순 스택으로 빠른 출시** – Next.js 14 + PostgreSQL + Redis 3종으로 MVP를 구축, 추후 AI 매칭·워크플로우 등 확장이 가능하도록 모듈형 구조를 확보한다.\n    \n\n---\n\n## 3. 주요 기능 (MVP 범위)\n\n|번호|기능명|설명|\n|---|---|---|\n|3-1|**통합 채용 피드**|워크투게더·고용24·사람인·잡코리아 공고를 최신순·필터별 리스트/상세로 제공|\n|3-2|**간단 사용자 프로필**|이메일·비밀번호·장애 등록 여부만 받는 경량 프로필, Saved Jobs·알림 설정 저장|\n|3-3|**기본 알림**|저장한 공고 상태 변화(마감‧수정) 및 신규 적합 공고 발생 시 이메일·푸시 전송|\n\n> **Phase 2 이후** – AI 매칭, 심리 체크-인, 상담 예약, 커뮤니티 등 고도화 기능은 차기 단계에서 추가한다.\n\n---\n\n## 4. 사용자 플로우 (MVP)\n\n```mermaid\nflowchart TD\n    A(회원가입/로그인) --> B(프로필 기본 정보 입력)\n    B --> C(통합 채용 피드 조회)\n    C -->|관심 공고 저장| D(내 저장 공고 목록)\n    C -->|공고 상세 열람| E(공고 상세)\n    D --> F(공고 마감·수정 시 알림 수신)\n```\n\n---\n\n## 5. 기술 요구사항 (MVP)\n\n### 5.1 프로젝트 구조 & 초기 설정\n\n```bash\nrebridge/\n├── apps/\n│   ├── web/                 # Next.js 14 App Router (SSR + API Routes)\n│   └── crawler/             # 독립 크롤러 서비스 (Node + Puppeteer)\n├── packages/\n│   ├── database/            # Prisma 스키마 및 클라이언트\n│   ├── shared/              # 공용 타입·유틸\n│   └── crawler-adapters/    # 사이트별 크롤러 로직\n└── docker-compose.yml       # 로컬 개발용 (PostgreSQL·Redis)\n```\n\n- **모노레포**(pnpm workspaces)로 코드·타입 공유.\n    \n- **CI/CD:** GitHub Actions → Vercel(웹/API) / Upstash Redis / Neon PostgreSQL.\n    \n\n---\n\n### 5.2 시스템 아키텍처\n\n```\n┌──────────────┐          ┌────────────────┐\n│   Next.js    │  GraphQL │  API Layer     │  Prisma ORM\n│ (Web+API)    │◀────────▶│  (App Router)  │────────────► PostgreSQL (Neon)\n└──────────────┘          └────────────────┘\n        ▲                           │\n        │                           ▼\n        │  WebSocket / REST   ┌──────────────┐\n        └─────────────────────►   Redis      │\n                                └────────────┘ (TTL 캐시 · Job Queue)\n                                    │\n                                    ▼\n                         ┌──────────────────────┐\n                         │  Crawler Manager     │\n                         └──────────────────────┘\n```\n\n---\n\n### 5.3 크롤링 서브시스템\n\n```typescript\n// packages/crawler-adapters/src/base.ts\nexport interface CrawlerAdapter {\n  source: 'workTogether' | 'work24' | 'saramin' | 'jobkorea';\n  crawl(page?: number): Promise<RawJobData[]>;\n  parseJobDetail(id: string): Promise<JobDetail>;\n  normalizeData(raw: RawJobData): NormalizedJob;\n}\n```\n\n```text\n크롤러 매니저\n├─ 스케줄러 (node-cron, 매 6 h)\n├─ 워커 풀 (Puppeteer, concurrency=4)\n│  ├─ WorkTogetherAdapter   { type:'static',  delay:3 s }\n│  ├─ Work24Adapter         { type:'dynamic', delay:5 s }\n│  ├─ SaraminAdapter        { type:'api',     delay:2 s }\n│  └─ JobKoreaAdapter       { type:'dynamic', delay:4 s }\n├─ 파서/정규화                (Cheerio + Zod)\n└─ DB 저장기                  (Prisma Tx)\n```\n\n- **레이트 리밋:** 요청 간격 2-5 초, IP Block 대비.\n    \n- **재시도:** 최대 3회 + 지수 Backoff(1→2→4 s).\n    \n- **중복 체크:** `(source, external_id)` 고유 키.\n    \n- **robots.txt 준수 및 User-Agent 식별**으로 법적·윤리적 이슈 예방.\n    \n\n---\n\n### 5.4 데이터베이스 스키마 (핵심 4 테이블)\n\n```sql\nCREATE TABLE users (\n    id                       UUID PRIMARY KEY,\n    email                    VARCHAR(255) UNIQUE NOT NULL,\n    password_hash            VARCHAR(255)        NOT NULL,\n    is_registered_disability BOOLEAN DEFAULT FALSE,\n    created_at               TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE jobs (\n    id                      UUID PRIMARY KEY,\n    source                  VARCHAR(50) NOT NULL,\n    external_id             VARCHAR(255) NOT NULL,\n    title                   TEXT NOT NULL,\n    company                 VARCHAR(255),\n    location_json           JSONB,\n    salary_range            JSONB,\n    employment_type         VARCHAR(50),\n    description             TEXT,\n    is_disability_friendly  BOOLEAN DEFAULT FALSE,\n    crawled_at              TIMESTAMP,\n    expires_at              TIMESTAMP,\n    raw_data                JSONB,\n    search_vector           TSVECTOR,\n    UNIQUE (source, external_id)\n);\n\nCREATE INDEX jobs_crawled_idx ON jobs (crawled_at DESC);\nCREATE INDEX jobs_company_title_idx ON jobs (company, title);\nCREATE INDEX jobs_search_idx ON jobs USING GIN(search_vector);\n\nCREATE TABLE user_saved_jobs (\n    user_id UUID REFERENCES users(id),\n    job_id  UUID REFERENCES jobs(id),\n    saved_at TIMESTAMP DEFAULT NOW(),\n    PRIMARY KEY(user_id, job_id)\n);\n\nCREATE TABLE crawl_logs (\n    id UUID PRIMARY KEY,\n    source VARCHAR(50),\n    status VARCHAR(20),\n    jobs_found INT,\n    jobs_new INT,\n    jobs_updated INT,\n    error_message TEXT,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP\n);\n```\n\n#### ➜ 전문 검색 최적화\n\n```sql\nCREATE TRIGGER jobs_search_vector_update\nBEFORE INSERT OR UPDATE ON jobs\nFOR EACH ROW EXECUTE FUNCTION\ntsvector_update_trigger(search_vector,\n                        'pg_catalog.korean',\n                        title, company, description);\n```\n\n---\n\n### 5.5 API & 화면 구현 (Next.js 14)\n\n```typescript\n// app/jobs/page.tsx  ─ 서버 컴포넌트 (SEO)\nexport default async function JobsPage({\n  searchParams\n}: { searchParams: { page?: string; q?: string } }) {\n  const page = Number(searchParams.page ?? 1);\n  const q = searchParams.q;\n\n  const jobs = await prisma.job.findMany({\n    where: q ? {\n      OR: [\n        { title:   { contains: q, mode: 'insensitive' } },\n        { company: { contains: q, mode: 'insensitive' } }\n      ]\n    } : undefined,\n    orderBy: { crawledAt: 'desc' },\n    take: 20,\n    skip: (page - 1) * 20\n  });\n\n  return <JobList jobs={jobs} />;\n}\n```\n\n- **App Router (Server Components)** 로 SEO, 캐싱(Tags) 최적화.\n    \n- **NextAuth.js** (Credentials + Kakao OAuth) 로 깔끔한 인증.\n    \n\n---\n\n### 5.6 알림 & 캐싱\n\n```typescript\n// Redis Pub/Sub Worker (BullMQ)\nconst notificationWorker = new Worker('notification', async job => {\n  const { userId, type, jobId } = job.data;\n  const user = await prisma.user.findUnique({ where: { id: userId } });\n\n  if (type === 'jobExpiring') {\n    await sendEmail({\n      to: user.email,\n      subject: '저장한 공고가 곧 마감됩니다',\n      html: /* … */\n    });\n  }\n});\n```\n\n- **캐싱** – `jobs:latest` 키에 최근 100건 JSON 저장, TTL 1 h\n    \n- **지속성** – Upstash Redis Durable Streams로 알림 이중화.\n    \n\n---\n\n### 5.7 성능 & 모니터링\n\n```typescript\n// jobs API with cache\nexport async function GET() {\n  const cached = await redis.get('jobs:latest');\n  if (cached) return Response.json(JSON.parse(cached));\n\n  const jobs = await prisma.job.findMany({\n    orderBy: { crawledAt: 'desc' },\n    take: 100\n  });\n  await redis.setex('jobs:latest', 3600, JSON.stringify(jobs));\n  return Response.json(jobs);\n}\n```\n\n```typescript\n// admin/crawl-status\nconst logs = await prisma.crawlLog.findMany({\n  orderBy: { startedAt: 'desc' },\n  take: 50\n});\nconst successRate = logs.filter(l => l.status === 'success').length / logs.length;\n```\n\n- **모니터링** – Grafana Cloud 대시보드 + Vercel Analytics.\n    \n- **Alert** – crawl 실패율 > 20 % or 평균 시간 > 15 min 시 Slack‧메일 알림.\n    \n\n---\n\n### 5.8 개발 우선순위 (4 주 계획)\n\n|주차|핵심 산출물|\n|---|---|\n|**Week 1**|모노레포 세팅, Prisma 스키마, 기본 인증(NextAuth)|\n|**Week 2**|WorkTogether·Saramin 크롤러, 데이터 정규화 & 저장 로직|\n|**Week 3**|채용 피드 UI/검색/저장, Redis 캐시|\n|**Week 4**|고용24·잡코리아 크롤러, 알림 시스템, 배포 & 모니터링|\n\n> **일정은 가이드라인**이며, 캘린더 구체 날짜는 별도 스프린트 보드에서 관리한다.\n\n---\n\n## 6. 기타 / 비고\n\n- **보안:** bcrypt(12) 해시, JWT + Refresh Token, TLS 1.3.\n    \n- **데이터 윤리:** robots.txt 준수, 원본 사이트 약관 범위 내 데이터 사용, “Right to Be Forgotten” 지원.\n    \n- **접근성:** WCAG 2.1 AA, 다크모드·키보드 내비게이션·스크린리더 라벨 완비.\n    \n- **향후 확장:** AI 직무 매칭·워크플로우 보드 등은 독립 **service package** 로 추가, API Gateway에서 GraphQL Federation으로 통합할 수 있도록 도메인 경계 유지.\n    \n\n**문서 종료.**",
      "writedAt": "2025-07-13T08:44:41.977Z"
    },
    {
      "type": "architecture",
      "content": "# 📐 기술 요구사항 문서 (TRD)\n\n## 1. 문서 개요\n\n- **제품명:** ReBridge\n- **작성일:** 2025-07-12\n- **버전:** 1.0 (MVP)\n- **목적:** PRD에 정의된 MVP 기능을 구현하기 위한 상세 기술 명세 및 구현 가이드\n\n---\n\n## 2. 기술 스택 상세\n\n### 2.1 핵심 기술 스택\n\n|계층|기술|버전|선택 이유|\n|---|---|---|---|\n|**Frontend**|Next.js|14.2.x|App Router, RSC, SEO 최적화|\n|**Backend**|Next.js API Routes|14.2.x|단일 코드베이스, Edge Runtime 지원|\n|**Database**|PostgreSQL (Neon)|16|Serverless, 자동 스케일링, 한국어 전문검색|\n|**Cache/Queue**|Upstash Redis|Latest|Serverless Redis, 낮은 레이턴시|\n|**ORM**|Prisma|5.x|타입 안전성, 마이그레이션 관리|\n|**크롤링**|Puppeteer/Playwright|Latest|동적 사이트 처리, 안정성|\n|**인증**|NextAuth.js|5.x|다양한 Provider 지원|\n|**모노레포**|pnpm + Turborepo|Latest|빠른 빌드, 효율적 캐싱|\n\n### 2.2 개발 도구\n\n```json\n{\n  \"typescript\": \"^5.3\",\n  \"eslint\": \"^8.57\",\n  \"prettier\": \"^3.2\",\n  \"vitest\": \"^1.2\",\n  \"playwright\": \"^1.41\"\n}\n```\n\n---\n\n## 3. 시스템 아키텍처 상세\n\n### 3.1 전체 아키텍처\n\n```mermaid\ngraph TB\n    subgraph \"Client Layer\"\n        A[Next.js SSR/CSR]\n        B[PWA Service Worker]\n    end\n    \n    subgraph \"API Layer\"\n        C[Next.js API Routes]\n        D[GraphQL Yoga]\n        E[NextAuth.js]\n    end\n    \n    subgraph \"Service Layer\"\n        F[Job Service]\n        G[User Service]\n        H[Notification Service]\n        I[Crawler Manager]\n    end\n    \n    subgraph \"Data Layer\"\n        J[(PostgreSQL)]\n        K[(Redis)]\n        L[Prisma ORM]\n    end\n    \n    subgraph \"External\"\n        M[WorkTogether]\n        N[고용24]\n        O[사람인]\n        P[잡코리아]\n    end\n    \n    A --> C\n    B --> C\n    C --> D\n    D --> F\n    D --> G\n    D --> H\n    F --> L\n    G --> L\n    H --> K\n    I --> L\n    I --> M\n    I --> N\n    I --> O\n    I --> P\n    L --> J\n    H --> K\n```\n\n### 3.2 데이터 흐름\n\n```typescript\n// 크롤링 → 저장 → 캐싱 → 제공\ninterface DataFlow {\n  1. \"크롤러가 외부 사이트에서 데이터 수집\"\n  2. \"어댑터가 데이터 정규화\"\n  3. \"Prisma로 PostgreSQL 저장\"\n  4. \"Redis에 최신 데이터 캐싱\"\n  5. \"API가 캐시 우선 조회 후 응답\"\n}\n```\n\n---\n\n## 4. API 설계\n\n### 4.1 GraphQL 스키마\n\n```graphql\ntype Query {\n  # 채용 공고 조회\n  jobs(\n    page: Int = 1\n    limit: Int = 20\n    search: String\n    filters: JobFilterInput\n  ): JobConnection!\n  \n  # 단일 공고 상세\n  job(id: ID!): Job\n  \n  # 내 저장 공고\n  savedJobs(page: Int = 1, limit: Int = 20): JobConnection!\n  \n  # 현재 사용자 정보\n  me: User\n}\n\ntype Mutation {\n  # 인증\n  signUp(input: SignUpInput!): AuthPayload!\n  signIn(input: SignInInput!): AuthPayload!\n  signOut: Boolean!\n  \n  # 공고 저장/삭제\n  saveJob(jobId: ID!): Job!\n  unsaveJob(jobId: ID!): Boolean!\n  \n  # 프로필 업데이트\n  updateProfile(input: UpdateProfileInput!): User!\n}\n\ntype Subscription {\n  # 새 공고 알림\n  newJobs(filters: JobFilterInput): Job!\n}\n\n# 타입 정의\ntype Job {\n  id: ID!\n  source: JobSource!\n  externalId: String!\n  title: String!\n  company: String\n  location: Location\n  salaryRange: SalaryRange\n  employmentType: String\n  description: String!\n  isDisabilityFriendly: Boolean!\n  crawledAt: DateTime!\n  expiresAt: DateTime\n  isSaved: Boolean!\n}\n\ntype JobConnection {\n  edges: [JobEdge!]!\n  pageInfo: PageInfo!\n  totalCount: Int!\n}\n\ninput JobFilterInput {\n  sources: [JobSource!]\n  isDisabilityFriendly: Boolean\n  employmentTypes: [String!]\n  locationIds: [String!]\n}\n\nenum JobSource {\n  WORK_TOGETHER\n  WORK24\n  SARAMIN\n  JOBKOREA\n}\n```\n\n### 4.2 REST API 엔드포인트\n\n```typescript\n// Health Check\nGET /api/health\n\n// 크롤러 Webhook\nPOST /api/crawler/trigger\n{\n  \"source\": \"workTogether\",\n  \"secret\": \"CRAWLER_SECRET\"\n}\n\n// 파일 업로드 (이력서 등)\nPOST /api/upload\nContent-Type: multipart/form-data\n```\n\n---\n\n## 5. 데이터베이스 설계\n\n### 5.1 Prisma 스키마\n\n```prisma\n// prisma/schema.prisma\ngenerator client {\n  provider = \"prisma-client-js\"\n  previewFeatures = [\"fullTextSearch\", \"postgresqlExtensions\"]\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url = env(\"DATABASE_URL\")\n  extensions = [pgcrypto, pg_trgm]\n}\n\nmodel User {\n  id                      String    @id @default(dbgenerated(\"gen_random_uuid()\"))\n  email                   String    @unique\n  passwordHash            String    @map(\"password_hash\")\n  isRegisteredDisability  Boolean   @default(false) @map(\"is_registered_disability\")\n  createdAt               DateTime  @default(now()) @map(\"created_at\")\n  updatedAt               DateTime  @updatedAt @map(\"updated_at\")\n  \n  profile                 Profile?\n  savedJobs              SavedJob[]\n  notifications          Notification[]\n  \n  @@map(\"users\")\n}\n\nmodel Profile {\n  userId          String   @id @map(\"user_id\")\n  name            String?\n  phoneNumber     String?  @map(\"phone_number\")\n  preferredAreas  Json?    @map(\"preferred_areas\")\n  emailAlerts     Boolean  @default(true) @map(\"email_alerts\")\n  pushAlerts      Boolean  @default(false) @map(\"push_alerts\")\n  \n  user            User     @relation(fields: [userId], references: [id], onDelete: Cascade)\n  \n  @@map(\"profiles\")\n}\n\nmodel Job {\n  id                    String    @id @default(dbgenerated(\"gen_random_uuid()\"))\n  source               String\n  externalId           String    @map(\"external_id\")\n  title                String\n  company              String?\n  locationJson         Json?     @map(\"location_json\")\n  salaryRange          Json?     @map(\"salary_range\")\n  employmentType       String?   @map(\"employment_type\")\n  description          String    @db.Text\n  isDisabilityFriendly Boolean   @default(false) @map(\"is_disability_friendly\")\n  crawledAt            DateTime  @map(\"crawled_at\")\n  expiresAt            DateTime? @map(\"expires_at\")\n  rawData              Json      @map(\"raw_data\")\n  \n  savedBy              SavedJob[]\n  \n  @@unique([source, externalId])\n  @@index([crawledAt(sort: Desc)])\n  @@index([company, title])\n  @@map(\"jobs\")\n}\n\nmodel SavedJob {\n  userId    String   @map(\"user_id\")\n  jobId     String   @map(\"job_id\")\n  savedAt   DateTime @default(now()) @map(\"saved_at\")\n  \n  user      User     @relation(fields: [userId], references: [id], onDelete: Cascade)\n  job       Job      @relation(fields: [jobId], references: [id], onDelete: Cascade)\n  \n  @@id([userId, jobId])\n  @@map(\"user_saved_jobs\")\n}\n\nmodel CrawlLog {\n  id            String    @id @default(dbgenerated(\"gen_random_uuid()\"))\n  source        String\n  status        String\n  jobsFound     Int       @map(\"jobs_found\")\n  jobsNew       Int       @map(\"jobs_new\")\n  jobsUpdated   Int       @map(\"jobs_updated\")\n  errorMessage  String?   @map(\"error_message\")\n  startedAt     DateTime  @map(\"started_at\")\n  completedAt   DateTime? @map(\"completed_at\")\n  \n  @@index([startedAt(sort: Desc)])\n  @@map(\"crawl_logs\")\n}\n\nmodel Notification {\n  id        String   @id @default(dbgenerated(\"gen_random_uuid()\"))\n  userId    String   @map(\"user_id\")\n  type      String\n  payload   Json\n  isRead    Boolean  @default(false) @map(\"is_read\")\n  createdAt DateTime @default(now()) @map(\"created_at\")\n  \n  user      User     @relation(fields: [userId], references: [id], onDelete: Cascade)\n  \n  @@index([userId, isRead])\n  @@map(\"notifications\")\n}\n```\n\n### 5.2 마이그레이션 전략\n\n```bash\n# 초기 마이그레이션\npnpm prisma migrate dev --name init\n\n# 전문검색 인덱스 추가\npnpm prisma migrate dev --name add_fulltext_search\n\n# 프로덕션 배포\npnpm prisma migrate deploy\n```\n\n---\n\n## 6. 크롤러 구현 상세\n\n### 6.1 크롤러 인터페이스\n\n```typescript\n// packages/crawler-adapters/src/types.ts\nexport interface RawJobData {\n  id: string;\n  title: string;\n  company?: string;\n  location?: string;\n  salary?: string;\n  type?: string;\n  description?: string;\n  postedAt?: string;\n  deadline?: string;\n  url: string;\n  raw: Record<string, unknown>;\n}\n\nexport interface NormalizedJob {\n  externalId: string;\n  title: string;\n  company: string | null;\n  locationJson: {\n    city?: string;\n    district?: string;\n    address?: string;\n  } | null;\n  salaryRange: {\n    min?: number;\n    max?: number;\n    type?: 'monthly' | 'yearly';\n  } | null;\n  employmentType: string | null;\n  description: string;\n  isDisabilityFriendly: boolean;\n  expiresAt: Date | null;\n  rawData: Record<string, unknown>;\n}\n\nexport abstract class BaseCrawler implements CrawlerAdapter {\n  abstract source: JobSource;\n  protected page: Page | null = null;\n  \n  abstract crawl(pageNum?: number): Promise<RawJobData[]>;\n  abstract parseJobDetail(id: string): Promise<JobDetail>;\n  abstract normalizeData(raw: RawJobData): NormalizedJob;\n  \n  protected async initBrowser(): Promise<void> {\n    const browser = await chromium.launch({\n      headless: true,\n      args: ['--no-sandbox', '--disable-setuid-sandbox']\n    });\n    this.page = await browser.newPage();\n    await this.page.setViewportSize({ width: 1920, height: 1080 });\n  }\n  \n  protected async delay(ms: number): Promise<void> {\n    await new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n```\n\n### 6.2 크롤러 스케줄링\n\n```typescript\n// apps/crawler/src/scheduler.ts\nimport { CronJob } from 'cron';\nimport { Queue } from 'bullmq';\n\nconst crawlQueue = new Queue('crawl-jobs', {\n  connection: redis\n});\n\n// 6시간마다 실행\nconst job = new CronJob('0 */6 * * *', async () => {\n  const sources = ['workTogether', 'work24', 'saramin', 'jobkorea'];\n  \n  for (const source of sources) {\n    await crawlQueue.add('crawl', { \n      source,\n      timestamp: new Date().toISOString()\n    }, {\n      attempts: 3,\n      backoff: {\n        type: 'exponential',\n        delay: 2000\n      }\n    });\n  }\n});\n\njob.start();\n```\n\n---\n\n## 7. 보안 요구사항\n\n### 7.1 인증 및 권한\n\n```typescript\n// lib/auth.ts\nexport const authOptions: NextAuthOptions = {\n  providers: [\n    CredentialsProvider({\n      credentials: {\n        email: { type: \"email\" },\n        password: { type: \"password\" }\n      },\n      async authorize(credentials) {\n        const user = await prisma.user.findUnique({\n          where: { email: credentials.email }\n        });\n        \n        if (!user) return null;\n        \n        const isValid = await bcrypt.compare(\n          credentials.password,\n          user.passwordHash\n        );\n        \n        if (!isValid) return null;\n        \n        return {\n          id: user.id,\n          email: user.email\n        };\n      }\n    }),\n    KakaoProvider({\n      clientId: process.env.KAKAO_CLIENT_ID!,\n      clientSecret: process.env.KAKAO_CLIENT_SECRET!\n    })\n  ],\n  session: {\n    strategy: 'jwt',\n    maxAge: 30 * 24 * 60 * 60 // 30일\n  },\n  jwt: {\n    secret: process.env.JWT_SECRET!\n  }\n};\n```\n\n### 7.2 보안 미들웨어\n\n```typescript\n// middleware.ts\nexport const config = {\n  matcher: ['/api/:path*', '/profile/:path*']\n};\n\nexport function middleware(request: NextRequest) {\n  // CSRF 토큰 검증\n  const token = request.headers.get('x-csrf-token');\n  if (!token || !verifyCSRFToken(token)) {\n    return new Response('Invalid CSRF token', { status: 403 });\n  }\n  \n  // Rate Limiting\n  const ip = request.ip ?? 'unknown';\n  const identifier = `${ip}:${request.url}`;\n  \n  // API 레이트 리밋: 분당 60회\n  if (isRateLimited(identifier, 60, 60)) {\n    return new Response('Too Many Requests', { status: 429 });\n  }\n  \n  return NextResponse.next();\n}\n```\n\n---\n\n## 8. 성능 요구사항\n\n### 8.1 성능 목표\n\n|메트릭|목표값|측정 방법|\n|---|---|---|\n|**페이지 로드**|< 2초 (LCP)|Lighthouse|\n|**API 응답**|< 200ms (p95)|Grafana|\n|**크롤링 주기**|6시간|CronJob|\n|**DB 쿼리**|< 50ms (p95)|Prisma Metrics|\n|**동시 사용자**|1,000명|K6 부하테스트|\n\n### 8.2 최적화 전략\n\n```typescript\n// 캐싱 전략\nconst cacheStrategy = {\n  jobs: {\n    list: 3600,      // 1시간\n    detail: 7200,    // 2시간\n    search: 1800     // 30분\n  },\n  user: {\n    profile: 300,    // 5분\n    savedJobs: 60    // 1분\n  }\n};\n\n// 데이터베이스 쿼리 최적화\nconst optimizedJobQuery = prisma.$queryRaw`\n  SELECT j.*, \n         EXISTS(SELECT 1 FROM user_saved_jobs WHERE job_id = j.id AND user_id = ${userId}) as is_saved\n  FROM jobs j\n  WHERE j.crawled_at > NOW() - INTERVAL '7 days'\n  ORDER BY j.crawled_at DESC\n  LIMIT 20 OFFSET ${offset}\n`;\n```\n\n---\n\n## 9. 배포 및 인프라\n\n### 9.1 배포 파이프라인\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: pnpm/action-setup@v2\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'pnpm'\n      \n      - run: pnpm install --frozen-lockfile\n      - run: pnpm test\n      - run: pnpm build\n      \n      - name: Deploy to Vercel\n        run: vercel --prod\n        env:\n          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}\n```\n\n### 9.2 환경 변수\n\n```env\n# .env.production\nDATABASE_URL=\nREDIS_URL=\nNEXTAUTH_URL=\nNEXTAUTH_SECRET=\nJWT_SECRET=\nKAKAO_CLIENT_ID=\nKAKAO_CLIENT_SECRET=\nCRAWLER_SECRET=\nRESEND_API_KEY=\nSENTRY_DSN=\n```\n\n---\n\n## 10. 모니터링 및 로깅\n\n### 10.1 로깅 전략\n\n```typescript\n// lib/logger.ts\nimport pino from 'pino';\n\nexport const logger = pino({\n  level: process.env.LOG_LEVEL || 'info',\n  transport: {\n    target: 'pino-pretty',\n    options: {\n      colorize: true\n    }\n  }\n});\n\n// 크롤링 로그\nlogger.info({\n  type: 'crawl_completed',\n  source: 'workTogether',\n  jobsFound: 150,\n  jobsNew: 23,\n  duration: 45000\n});\n```\n\n### 10.2 모니터링 대시보드\n\n```typescript\n// 주요 메트릭\ninterface Metrics {\n  // 크롤링\n  crawlSuccessRate: number;\n  avgCrawlDuration: number;\n  totalJobsCrawled: number;\n  \n  // API\n  apiRequestRate: number;\n  apiErrorRate: number;\n  apiResponseTime: number;\n  \n  // 사용자\n  activeUsers: number;\n  newSignups: number;\n  savedJobsCount: number;\n}\n```\n\n---\n\n## 11. 테스트 전략\n\n### 11.1 테스트 구조\n\n```typescript\n// 단위 테스트 예시\ndescribe('JobNormalizer', () => {\n  it('should normalize salary range correctly', () => {\n    const raw = { salary: '300-400만원' };\n    const normalized = normalizer.normalizeSalary(raw);\n    \n    expect(normalized).toEqual({\n      min: 3000000,\n      max: 4000000,\n      type: 'monthly'\n    });\n  });\n});\n\n// E2E 테스트\ntest('user can save and view jobs', async ({ page }) => {\n  await page.goto('/jobs');\n  await page.click('[data-testid=\"job-save-button\"]');\n  await page.goto('/saved-jobs');\n  \n  await expect(page.locator('[data-testid=\"saved-job-item\"]')).toBeVisible();\n});\n```\n\n---\n\n## 12. 기술적 위험 및 대응\n\n|위험 요소|영향도|대응 방안|\n|---|---|---|\n|**크롤링 차단**|높음|User-Agent 로테이션, 프록시 준비|\n|**데이터 정규화 실패**|중간|원본 데이터 보존, 수동 검수|\n|**트래픽 급증**|중간|Auto-scaling, CDN 활용|\n|**보안 취약점**|높음|정기 보안 감사, OWASP 체크|\n\n---\n\n## 13. 개발 규칙\n\n### 13.1 코드 스타일\n\n```typescript\n// ESLint 설정\n{\n  \"extends\": [\"next/core-web-vitals\", \"prettier\"],\n  \"rules\": {\n    \"no-console\": [\"error\", { \"allow\": [\"warn\", \"error\"] }],\n    \"@typescript-eslint/explicit-function-return-type\": \"error\"\n  }\n}\n```\n\n### 13.2 커밋 규칙\n\n```bash\n# Conventional Commits\nfeat: 새로운 기능\nfix: 버그 수정\ndocs: 문서 수정\nstyle: 코드 포맷팅\nrefactor: 코드 리팩토링\ntest: 테스트 추가/수정\nchore: 빌드/설정 수정\n```\n\n---\n",
      "writedAt": "2025-07-13T08:44:41.977Z"
    },
    {
      "type": "guideline",
      "content": "# Code Guideline for ReBridge\n\n## 1. Project Overview  \n- Monorepo (pnpm workspaces) with:  \n  - `apps/web` – Next.js 14 App Router (SSR/Server Components) + API Routes + GraphQL  \n  - `apps/crawler` – Node.js + Puppeteer crawler service (Cron + BullMQ)  \n  - `packages/database` – Prisma schema & client singleton  \n  - `packages/shared` – shared types & utils  \n  - `packages/crawler-adapters` – site-specific crawler logic  \n- Data flow: Crawler → Zod parsing → Prisma → Upstash Redis (TTL cache, BullMQ queues) → Next.js API/GraphQL → Frontend  \n- CI/CD: GitHub Actions → Vercel (Web/API), Neon PostgreSQL, Upstash Redis  \n- Key patterns: Server Components for SEO/caching, cache-first queries, modular feature folders, strict error contracts  \n\n## 2. Core Principles  \n1. **Type Safety** – All exported functions/components MUST declare explicit TypeScript types.  \n2. **Single Responsibility** – Modules/functions ≤ 200 LOC and address one concern.  \n3. **Consistent Error Handling** – Wrap all async logic in `try/catch` and return standardized `ApiError`.  \n4. **Resource Efficiency** – Batch DB writes and cache reads; measure with query-count metrics.  \n5. **Modularity & Reuse** – Shared utilities and types MUST reside in shared packages; no duplication.\n\n## 3. Language-Specific Guidelines\n\n### 3.1 TypeScript & Next.js  \n- File organization:  \n  - `apps/web/app/feature-name/` containing `page.tsx`, `route.ts`, `Component.tsx`  \n  - `components/` for UI atoms/molecules, `features/` for pages  \n- Imports:  \n  - Use absolute paths via `tsconfig.json` `baseUrl`/`paths`  \n  - Avoid `../../../`; max two relative levels  \n- Error handling:  \n  - API routes/GraphQL resolvers MUST throw/return `ApiError({ status, code, message })`  \n  - Use Next.js `error.js` boundaries for UI\n\n### 3.2 Prisma & Database  \n- Client: export a singleton `prisma` from `packages/database/client.ts`  \n- Transactions: use `prisma.$transaction([...])` for atomic operations  \n- Migrations:  \n  - Dev: `pnpm prisma migrate dev --name <desc>`  \n  - Prod: `pnpm prisma migrate deploy`\n\n### 3.3 GraphQL  \n- Schema in `packages/graphql/schema/`, resolvers in `packages/graphql/resolvers/`  \n- Business logic resides in `services/` modules; resolvers delegate to services  \n- Context must provide `{ prisma, user }`; propagate errors via `AuthenticationError` or `UserInputError`\n\n### 3.4 Crawler (Node & Puppeteer)  \n- Adapters extend `BaseCrawler` in `packages/crawler-adapters`  \n- Schedule in `apps/crawler/src/scheduler.ts` using Cron + BullMQ (concurrency=4)  \n- Enforce rate limits (2–5 s delay) and retries (max 3, exponential backoff)  \n- Validate raw data with Zod before normalization\n\n### 3.5 Redis & Caching  \n- Key conventions:  \n  - Lists: `jobs:latest`, `jobs:search:<query>`  \n  - User: `user:<id>:saved-jobs`  \n- Use `redis.setex(key, ttl, value)` for TTL  \n- Use BullMQ for persistent queues; avoid raw Pub/Sub for critical flows\n\n## 4. Code Style Rules\n\n### MUST Follow  \n- ESLint + Prettier: extend `next/core-web-vitals` & `prettier`  \n- TS strict mode: `strict: true`, `noImplicitAny: true`  \n- Validation: use Zod for all external inputs (API, crawler)  \n- Naming: domain-driven, camelCase for variables/functions, PascalCase for types/components  \n- JSDoc: all public APIs/classes MUST have a brief JSDoc\n\n### MUST NOT Do  \n- `console.log` in production; use `logger.info`/`logger.error` (pino)  \n- Modules > 200 LOC or > 3 nested callback levels  \n- Mix UI and business logic in the same file  \n- Inline CSS/global styles; use CSS Modules or Tailwind\n\n## 5. Architecture Patterns\n\n### Component & Module Structure  \n- Feature-based folders: `features/jobs/`, `features/user/`  \n- Shared code: `packages/shared/` for hooks, types, util  \n- Services: `services/jobService.ts`, `services/notificationService.ts`\n\n### Data Flow Patterns  \n- Crawler → Zod parse → Prisma write → Redis cache → API → Frontend  \n- Cache-first: list/detail endpoints check Redis before DB\n\n### State Management Conventions  \n- Server Components for data fetching (SEO, caching)  \n- Client Components only for interactive UI; use SWR or React Query for client cache\n\n### API Design Standards  \n- GraphQL for complex queries/mutations; REST only for webhooks (`/api/crawler/trigger`) and file uploads  \n- Error response shape:  \n  ```json\n  { \"error\": { \"code\":\"BAD_REQUEST\", \"message\":\"Invalid input\", \"details\":{} } }\n  ```\n\n## 6. Example Code Snippets\n\n```typescript\n// MUST: Singleton Prisma client (packages/database/client.ts)\nimport { PrismaClient } from '@prisma/client';\nconst prisma = global.prisma ?? new PrismaClient();\nif (process.env.NODE_ENV === 'development') global.prisma = prisma;\nexport default prisma;\n```\n\n```typescript\n// MUST NOT: Instantiating PrismaClient per request\nexport async function handler(req, res) {\n  const prisma = new PrismaClient(); // ❌ connection storm\n  const jobs = await prisma.job.findMany();\n  res.json(jobs);\n}\n```\n\n```typescript\n// MUST: Zod validation of crawler data\nimport { z } from 'zod';\nconst RawJobSchema = z.object({\n  id: z.string(), title: z.string(), url: z.string(), raw: z.record(z.unknown())\n});\nexport type RawJobData = z.infer<typeof RawJobSchema>;\n```\n\n```typescript\n// MUST NOT: Using unvalidated raw data\nconst rawJobs: any[] = await crawler.crawl();\nawait prisma.job.createMany({ data: rawJobs }); // ❌ shape mismatch risk\n```\n\n```tsx\n// MUST: Server Component with data fetch\nexport default async function JobsPage() {\n  const jobs = await prisma.job.findMany({ orderBy: { crawledAt: 'desc' }, take:20 });\n  return <JobList jobs={jobs} />;\n}\n```\n\n```tsx\n// MUST NOT: Client Component for static data\n'use client';\nexport default function JobsPage() {\n  const [jobs, setJobs] = useState([]);\n  useEffect(() => fetch('/api/jobs').then(r => r.json()).then(setJobs));\n  return <JobList jobs={jobs}/>;\n}\n```\n\n```graphql\n# MUST: Modular GraphQL schema (packages/graphql/schema/job.graphql)\ntype Job { id: ID! title: String! company: String }\n```\n\n```typescript\n// MUST NOT: Business logic in resolver\nconst resolvers = {\n  Query: {\n    jobs: () => prisma.job.findMany() // ❌ move to jobService.fetchJobs()\n  }\n};\n```\n\n```typescript\n// MUST: API error handling\nexport async function GET(request: Request) {\n  try {\n    const jobs = await getLatestJobs();\n    return new Response(JSON.stringify(jobs), { status: 200 });\n  } catch (err) {\n    return new Response(JSON.stringify({ error: { code:'INTERNAL', message: err.message }}), { status:500 });\n  }\n}\n```\n\n```typescript\n// MUST NOT: Uncaught errors in route\nexport async function GET(request: Request) {\n  const jobs = await getLatestJobs(); // ❌ uncaught exceptions cause 500 without shape\n  return new Response(JSON.stringify(jobs));\n}\n```\n\n## 7. Testing Standards\n\n### 7.1 Test Coverage Requirements\n```typescript\n// MUST: Minimum coverage targets\n{\n  \"statements\": 80,\n  \"branches\": 75,\n  \"functions\": 80,\n  \"lines\": 80\n}\n```\n\n### 7.2 Test File Structure\n```typescript\n// MUST: Co-locate tests with source files\nsrc/\n  components/\n    JobCard.tsx\n    JobCard.test.tsx    // Unit tests\n    JobCard.stories.tsx // Storybook stories\n```\n\n### 7.3 Test Patterns\n```typescript\n// MUST: Use Testing Library best practices\nimport { render, screen } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\n\ntest('should save job when save button clicked', async () => {\n  const user = userEvent.setup();\n  const onSave = jest.fn();\n  \n  render(<JobCard job={mockJob} onSave={onSave} />);\n  \n  await user.click(screen.getByRole('button', { name: /저장/i }));\n  \n  expect(onSave).toHaveBeenCalledWith(mockJob.id);\n});\n\n// MUST NOT: Test implementation details\ntest('should set state to true', () => {\n  // ❌ Testing internal state instead of behavior\n  expect(component.state.isSaved).toBe(true);\n});\n```\n\n### 7.4 Crawler Testing\n```typescript\n// MUST: Mock external dependencies\nimport { mockPage } from '@/test-utils/mock-puppeteer';\n\nbeforeEach(() => {\n  jest.spyOn(global, 'fetch').mockResolvedValue({\n    ok: true,\n    json: async () => mockJobData\n  });\n});\n\n// MUST: Test error scenarios\ntest('should retry on network failure', async () => {\n  mockPage.goto.mockRejectedValueOnce(new Error('Network error'));\n  mockPage.goto.mockResolvedValueOnce();\n  \n  await crawler.crawl();\n  \n  expect(mockPage.goto).toHaveBeenCalledTimes(2);\n});\n```\n\n## 8. Security Guidelines\n\n### 8.1 Input Validation\n```typescript\n// MUST: Validate all user inputs\nimport { z } from 'zod';\n\nconst LoginSchema = z.object({\n  email: z.string().email().max(255),\n  password: z.string().min(8).max(100)\n});\n\nexport async function POST(request: Request) {\n  const body = await request.json();\n  const validation = LoginSchema.safeParse(body);\n  \n  if (!validation.success) {\n    return new Response(JSON.stringify({\n      error: { code: 'VALIDATION_ERROR', details: validation.error.errors }\n    }), { status: 400 });\n  }\n}\n\n// MUST NOT: Trust user input\nconst query = `SELECT * FROM users WHERE email = '${req.body.email}'`; // ❌ SQL injection\n```\n\n### 8.2 Authentication & Authorization\n```typescript\n// MUST: Check permissions in API routes\nimport { getServerSession } from 'next-auth';\n\nexport async function DELETE(request: Request, { params }: { params: { id: string } }) {\n  const session = await getServerSession(authOptions);\n  \n  if (!session?.user) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n  \n  // Check ownership\n  const job = await prisma.savedJob.findFirst({\n    where: { jobId: params.id, userId: session.user.id }\n  });\n  \n  if (!job) {\n    return new Response('Forbidden', { status: 403 });\n  }\n}\n```\n\n### 8.3 Sensitive Data Handling\n```typescript\n// MUST: Exclude sensitive fields\nconst user = await prisma.user.findUnique({\n  where: { id },\n  select: {\n    id: true,\n    email: true,\n    profile: true,\n    // passwordHash: false, // Never expose\n  }\n});\n\n// MUST: Use environment variables\nconst apiKey = process.env.CRAWLER_API_KEY!;\n// MUST NOT: Hardcode secrets\nconst apiKey = 'sk-1234567890'; // ❌\n```\n\n## 9. Performance Guidelines\n\n### 9.1 Database Optimization\n```typescript\n// MUST: Use select to limit fields\nconst jobs = await prisma.job.findMany({\n  select: {\n    id: true,\n    title: true,\n    company: true,\n    // Exclude large description field for list views\n  },\n  take: 20\n});\n\n// MUST: Use pagination\nconst { page = 1, limit = 20 } = request.query;\nconst jobs = await prisma.job.findMany({\n  skip: (page - 1) * limit,\n  take: limit\n});\n\n// MUST NOT: Fetch all records\nconst allJobs = await prisma.job.findMany(); // ❌ Memory issues\n```\n\n### 9.2 Caching Strategy\n```typescript\n// MUST: Implement cache layers\nexport async function getLatestJobs() {\n  // 1. Check Redis cache\n  const cached = await redis.get('jobs:latest');\n  if (cached) return JSON.parse(cached);\n  \n  // 2. Fetch from DB\n  const jobs = await prisma.job.findMany({\n    orderBy: { crawledAt: 'desc' },\n    take: 100\n  });\n  \n  // 3. Cache for 1 hour\n  await redis.setex('jobs:latest', 3600, JSON.stringify(jobs));\n  \n  return jobs;\n}\n\n// MUST: Invalidate cache on updates\nexport async function createJob(data: JobInput) {\n  const job = await prisma.job.create({ data });\n  await redis.del('jobs:latest'); // Invalidate\n  return job;\n}\n```\n\n### 9.3 Image Optimization\n```typescript\n// MUST: Use Next.js Image component\nimport Image from 'next/image';\n\n<Image\n  src={company.logo}\n  alt={`${company.name} 로고`}\n  width={100}\n  height={100}\n  loading=\"lazy\"\n/>\n\n// MUST NOT: Use unoptimized images\n<img src={company.logo} /> // ❌\n```\n\n## 10. Logging Standards\n\n### 10.1 Structured Logging\n```typescript\n// MUST: Use structured logging with context\nimport { logger } from '@/lib/logger';\n\nlogger.info('Job crawling completed', {\n  source: 'workTogether',\n  jobsFound: 150,\n  jobsNew: 23,\n  duration: 45000,\n  timestamp: new Date().toISOString()\n});\n\n// MUST NOT: Use console.log\nconsole.log('Found jobs:', jobs); // ❌\n```\n\n### 10.2 Log Levels\n```typescript\n// Log level usage:\nlogger.debug('Detailed debug info', { query, params }); // Development only\nlogger.info('Normal operations', { userId, action });   // General info\nlogger.warn('Warning conditions', { retries, delay });  // Potential issues\nlogger.error('Error occurred', { error, stack });       // Recoverable errors\nlogger.fatal('System failure', { error });              // Unrecoverable errors\n```\n\n### 10.3 Sensitive Data in Logs\n```typescript\n// MUST: Sanitize sensitive data\nlogger.info('User login', {\n  email: user.email,\n  // password: '***', // Never log passwords\n  ip: request.ip\n});\n\n// MUST: Use log redaction\nconst sanitizedUser = {\n  ...user,\n  passwordHash: undefined,\n  phoneNumber: user.phoneNumber?.replace(/\\d(?=\\d{4})/g, '*')\n};\n```\n\n## 11. Accessibility Guidelines\n\n### 11.1 Component Accessibility\n```tsx\n// MUST: Provide accessible markup\n<button\n  onClick={handleSave}\n  aria-label={isSaved ? '저장 취소' : '공고 저장'}\n  aria-pressed={isSaved}\n>\n  <Icon name={isSaved ? 'bookmark-filled' : 'bookmark'} />\n</button>\n\n// MUST: Use semantic HTML\n<nav aria-label=\"채용 공고 페이지네이션\">\n  <ul role=\"list\">...</ul>\n</nav>\n\n// MUST NOT: Use divs for interactive elements\n<div onClick={handleClick}>클릭하세요</div> // ❌\n```\n\n### 11.2 Form Accessibility\n```tsx\n// MUST: Associate labels with inputs\n<label htmlFor=\"email\">이메일</label>\n<input \n  id=\"email\"\n  type=\"email\"\n  required\n  aria-describedby=\"email-error\"\n/>\n{error && <span id=\"email-error\" role=\"alert\">{error}</span>}\n\n// MUST: Provide keyboard navigation\nonKeyDown={(e) => {\n  if (e.key === 'Enter' || e.key === ' ') {\n    e.preventDefault();\n    handleSelect();\n  }\n}}\n```\n\n## 12. Version Control & Git\n\n### 12.1 Branch Strategy\n```bash\n# Branch naming\nmain                    # Production\ndevelop                 # Development\nfeature/job-search      # New features\nfix/crawler-timeout     # Bug fixes\nhotfix/security-patch   # Emergency fixes\n```\n\n### 12.2 Commit Messages\n```bash\n# Format: <type>(<scope>): <subject>\nfeat(jobs): add advanced search filters\nfix(crawler): handle timeout errors\ndocs(readme): update installation steps\nperf(api): optimize job query performance\ntest(auth): add login integration tests\n```\n\n### 12.3 Pre-commit Hooks\n```json\n// .husky/pre-commit\n{\n  \"hooks\": {\n    \"pre-commit\": \"lint-staged\",\n    \"commit-msg\": \"commitlint -E HUSKY_GIT_PARAMS\"\n  }\n}\n\n// lint-staged.config.js\n{\n  \"*.{ts,tsx}\": [\"eslint --fix\", \"prettier --write\"],\n  \"*.test.{ts,tsx}\": [\"jest --bail --findRelatedTests\"]\n}\n```\n\n## 13. Environment Management\n\n### 13.1 Environment Variables\n```typescript\n// MUST: Type environment variables\n// env.d.ts\ndeclare namespace NodeJS {\n  interface ProcessEnv {\n    DATABASE_URL: string;\n    REDIS_URL: string;\n    NEXTAUTH_SECRET: string;\n    CRAWLER_SECRET: string;\n  }\n}\n\n// MUST: Validate at runtime\nconst requiredEnvVars = [\n  'DATABASE_URL',\n  'REDIS_URL',\n  'NEXTAUTH_SECRET'\n];\n\nrequiredEnvVars.forEach(varName => {\n  if (!process.env[varName]) {\n    throw new Error(`Missing required environment variable: ${varName}`);\n  }\n});\n```\n\n### 13.2 Secret Management\n```bash\n# .env.local (git ignored)\nDATABASE_URL=postgresql://...\nREDIS_URL=redis://...\n\n# .env.example (committed)\nDATABASE_URL=postgresql://user:pass@localhost:5432/db\nREDIS_URL=redis://localhost:6379\n```\n\n## 14. Code Review Checklist\n\n### Before Submitting PR\n- [ ] All tests pass (`pnpm test`)\n- [ ] No linting errors (`pnpm lint`)\n- [ ] Type checks pass (`pnpm type-check`)\n- [ ] Coverage meets minimum requirements\n- [ ] No console.log statements\n- [ ] Sensitive data is not exposed\n- [ ] Error handling is implemented\n- [ ] Loading and error states are handled in UI\n- [ ] Accessibility requirements are met\n- [ ] Performance impact is considered\n\n### Review Focus Areas\n1. **Security**: Input validation, authentication, authorization\n2. **Performance**: Query optimization, caching, lazy loading\n3. **Error Handling**: Try-catch blocks, error boundaries, user feedback\n4. **Code Quality**: Single responsibility, DRY principle, readability\n5. **Testing**: Edge cases, error scenarios, integration points\n\n## 15. Performance Monitoring\n\n### 15.1 Web Vitals\n```typescript\n// MUST: Monitor Core Web Vitals\nimport { getCLS, getFID, getFCP, getLCP, getTTFB } from 'web-vitals';\n\nfunction sendToAnalytics(metric: Metric) {\n  const body = JSON.stringify(metric);\n  \n  if (navigator.sendBeacon) {\n    navigator.sendBeacon('/analytics', body);\n  }\n}\n\ngetCLS(sendToAnalytics);\ngetFID(sendToAnalytics);\ngetFCP(sendToAnalytics);\ngetLCP(sendToAnalytics);\ngetTTFB(sendToAnalytics);\n```\n\n### 15.2 API Performance\n```typescript\n// MUST: Add performance timing\nexport async function GET(request: Request) {\n  const start = performance.now();\n  \n  try {\n    const result = await fetchData();\n    const duration = performance.now() - start;\n    \n    logger.info('API request completed', {\n      path: request.url,\n      duration,\n      resultCount: result.length\n    });\n    \n    return Response.json(result);\n  } catch (error) {\n    const duration = performance.now() - start;\n    logger.error('API request failed', { error, duration });\n    throw error;\n  }\n}\n```",
      "writedAt": "2025-07-13T08:44:41.977Z"
    },
    {
      "type": "step-by-step",
      "content": "\n## Core Directive\nYou are a senior software engineer AI assistant. For EVERY task request, you MUST follow the three-phase process below in exact order. Each phase must be completed with expert-level precision and detail.\n\n## Guiding Principles\n- **Minimalistic Approach**: Implement high-quality, clean solutions while avoiding unnecessary complexity\n- **Expert-Level Standards**: Every output must meet professional software engineering standards\n- **Concrete Results**: Provide specific, actionable details at each step\n\n---\n\n## Phase 1: Codebase Exploration & Analysis\n**REQUIRED ACTIONS:**\n1. **Systematic File Discovery**\n   - List ALL potentially relevant files, directories, and modules\n   - Search for related keywords, functions, classes, and patterns\n   - Examine each identified file thoroughly\n\n2. **Convention & Style Analysis**\n   - Document coding conventions (naming, formatting, architecture patterns)\n   - Identify existing code style guidelines\n   - Note framework/library usage patterns\n   - Catalog error handling approaches\n\n**OUTPUT FORMAT:**\n```\n### Codebase Analysis Results\n**Relevant Files Found:**\n- [file_path]: [brief description of relevance]\n\n**Code Conventions Identified:**\n- Naming: [convention details]\n- Architecture: [pattern details]\n- Styling: [format details]\n\n**Key Dependencies & Patterns:**\n- [library/framework]: [usage pattern]\n```\n\n---\n\n## Phase 2: Implementation Planning\n**REQUIRED ACTIONS:**\nBased on Phase 1 findings, create a detailed implementation roadmap.\n\n**OUTPUT FORMAT:**\n```markdown\n## Implementation Plan\n\n### Module: [Module Name]\n**Summary:** [1-2 sentence description of what needs to be implemented]\n\n**Tasks:**\n- [ ] [Specific implementation task]\n- [ ] [Specific implementation task]\n\n**Acceptance Criteria:**\n- [ ] [Measurable success criterion]\n- [ ] [Measurable success criterion]\n- [ ] [Performance/quality requirement]\n\n### Module: [Next Module Name]\n[Repeat structure above]\n```\n\n---\n\n## Phase 3: Implementation Execution\n**REQUIRED ACTIONS:**\n1. Implement each module following the plan from Phase 2\n2. Verify ALL acceptance criteria are met before proceeding\n3. Ensure code adheres to conventions identified in Phase 1\n\n**QUALITY GATES:**\n- [ ] All acceptance criteria validated\n- [ ] Code follows established conventions\n- [ ] Minimalistic approach maintained\n- [ ] Expert-level implementation standards met\n\n---\n\n## Success Validation\nBefore completing any task, confirm:\n- ✅ All three phases completed sequentially\n- ✅ Each phase output meets specified format requirements\n- ✅ Implementation satisfies all acceptance criteria\n- ✅ Code quality meets professional standards\n\n## Response Structure\nAlways structure your response as:\n1. **Phase 1 Results**: [Codebase analysis findings]\n2. **Phase 2 Plan**: [Implementation roadmap]  \n3. **Phase 3 Implementation**: [Actual code with validation]\n",
      "writedAt": "2025-07-13T08:44:41.977Z"
    },
    {
      "type": "clean-code",
      "content": "\n# Clean Code Guidelines\n\nYou are an expert software engineer focused on writing clean, maintainable code. Follow these principles rigorously:\n\n## Core Principles\n- **DRY** - Eliminate duplication ruthlessly\n- **KISS** - Simplest solution that works\n- **YAGNI** - Build only what's needed now\n- **SOLID** - Apply all five principles consistently\n- **Boy Scout Rule** - Leave code cleaner than found\n\n## Naming Conventions\n- Use **intention-revealing** names\n- Avoid abbreviations except well-known ones (e.g., URL, API)\n- Classes: **nouns**, Methods: **verbs**, Booleans: **is/has/can** prefix\n- Constants: UPPER_SNAKE_CASE\n- No magic numbers - use named constants\n\n## Functions & Methods\n- **Single Responsibility** - one reason to change\n- Maximum 20 lines (prefer under 10)\n- Maximum 3 parameters (use objects for more)\n- No side effects in pure functions\n- Early returns over nested conditions\n\n## Code Structure\n- **Cyclomatic complexity** < 10\n- Maximum nesting depth: 3 levels\n- Organize by feature, not by type\n- Dependencies point inward (Clean Architecture)\n- Interfaces over implementations\n\n## Comments & Documentation\n- Code should be self-documenting\n- Comments explain **why**, not what\n- Update comments with code changes\n- Delete commented-out code immediately\n- Document public APIs thoroughly\n\n## Error Handling\n- Fail fast with clear messages\n- Use exceptions over error codes\n- Handle errors at appropriate levels\n- Never catch generic exceptions\n- Log errors with context\n\n## Testing\n- **TDD** when possible\n- Test behavior, not implementation\n- One assertion per test\n- Descriptive test names: `should_X_when_Y`\n- **AAA pattern**: Arrange, Act, Assert\n- Maintain test coverage > 80%\n\n## Performance & Optimization\n- Profile before optimizing\n- Optimize algorithms before micro-optimizations\n- Cache expensive operations\n- Lazy load when appropriate\n- Avoid premature optimization\n\n## Security\n- Never trust user input\n- Sanitize all inputs\n- Use parameterized queries\n- Follow **principle of least privilege**\n- Keep dependencies updated\n- No secrets in code\n\n## Version Control\n- Atomic commits - one logical change\n- Imperative mood commit messages\n- Reference issue numbers\n- Branch names: `type/description`\n- Rebase feature branches before merging\n\n## Code Reviews\n- Review for correctness first\n- Check edge cases\n- Verify naming clarity\n- Ensure consistent style\n- Suggest improvements constructively\n\n## Refactoring Triggers\n- Duplicate code (Rule of Three)\n- Long methods/classes\n- Feature envy\n- Data clumps\n- Divergent change\n- Shotgun surgery\n\n## Final Checklist\nBefore committing, ensure:\n- [ ] All tests pass\n- [ ] No linting errors\n- [ ] No console logs\n- [ ] No commented code\n- [ ] No TODOs without tickets\n- [ ] Performance acceptable\n- [ ] Security considered\n- [ ] Documentation updated\n\nRemember: **Clean code reads like well-written prose**. Optimize for readability and maintainability over cleverness.\n",
      "writedAt": "2025-07-13T08:44:41.977Z"
    },
    {
      "type": "git-commit-message",
      "content": "\n# Git Commit Message Rules\n\n## Format Structure\n```\n<type>(<scope>): <description>\n\n[optional body]\n\n[optional footer]\n```\n\n## Types (Required)\n- `feat`\n- `fix`\n- `docs`\n- `style`\n- `refactor`\n- `perf`\n- `test`\n- `chore`\n- `ci`\n- `build`\n- `revert`\n\n## Scope (Optional)\n- Component, file, or feature area affected\n- Use kebab-case: `user-auth`, `payment-api`\n- Omit if change affects multiple areas\n\n## Description Rules\n- Use imperative mood\n- No capitalization of first letter\n- No period at end\n- Max 50 characters\n- Be specific and actionable\n\n## Body Guidelines\n- Wrap at 72 characters\n- Explain what and why, not how\n- Separate from description with blank line\n- Use bullet points for multiple changes\n\n## Footer Format\n- `BREAKING CHANGE:` for breaking changes\n- `Closes #123` for issue references\n- `Co-authored-by: Vooster AI (@vooster-ai)`\n\n## Examples\n```\nfeat(auth): add OAuth2 Google login\n\nfix: resolve memory leak in user session cleanup\n\ndocs(api): update authentication endpoints\n\nrefactor(utils): extract validation helpers to separate module\n\nBREAKING CHANGE: remove deprecated getUserData() method\n```\n\n## Workflow Integration\n**ALWAYS write a commit message after completing any development task, feature, or bug fix.**\n\n## Validation Checklist\n- [ ] Type is from approved list\n- [ ] Description under 50 chars\n- [ ] Imperative mood used\n- [ ] No trailing period\n- [ ] Meaningful and clear context\n    ",
      "writedAt": "2025-07-13T08:44:41.977Z"
    }
  ]
}